<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_logo.png" width="400"/>
<p> 
<p align="center">&nbsp&nbspğŸ’œ <a href="https://chat.qwen.ai/">Qwen Chat</a>&nbsp&nbsp |
           &nbsp&nbspğŸ¤— <a href="https://huggingface.co/Qwen/Qwen-Image">HuggingFace(T2I)</a>&nbsp&nbsp |
           &nbsp&nbspğŸ¤— <a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511">HuggingFace(Edit)</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/models/Qwen/Qwen-Image">ModelScope-T2I</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511">ModelScope-Edit</a>&nbsp&nbsp| &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2508.02324">Tech Report</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://qwenlm.github.io/blog/qwen-image/">Blog(T2I)</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://qwenlm.github.io/blog/qwen-image-edit-2511/">Blog(Edit)</a> &nbsp&nbsp 
<br>
ğŸ–¥ï¸ <a href="https://huggingface.co/spaces/Qwen/Qwen-Image">T2I Demo</a>&nbsp&nbsp | ğŸ–¥ï¸ <a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit-2511">Edit Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href="https://github.com/QwenLM/Qwen-Image/blob/main/assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp
</p>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/merge3.jpg" width="1024"/>
<p>

## Introduction
We are thrilled to release **Qwen-Image**, a 20B MMDiT image foundation model that achieves significant advances in **complex text rendering** and **precise image editing**. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.


![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center)

## News
- 2025.12.31: We released Qwen-Image-2512 weights! Check at [Huggingface](https://huggingface.co/Qwen/Qwen-Image-2512) and [ModelScope](https://modelscope.cn/models/Qwen/Qwen-Image-2512)!
- 2025.12.31: We released Qwen-Image-2512! Check our [Blog](https://qwen.ai/blog?id=qwen-image-2512) for more details!
    ğŸš€ Our December upgrade to Qwen-Image, just in time for the New Year.

    âœ¨ Whatâ€™s new:
    â€¢ More realistic humans â€” dramatically reduced â€œAI look,â€ richer facial & age details
    â€¢ Finer natural textures â€” sharper landscapes, water, fur, and materials
    â€¢ Stronger text rendering â€” better layout, higher accuracy in textâ€“image composition

    ğŸ† Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems.
    ![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/arena.png#center)
- 2025.12.31: [Qwen-Image-Lightning](https://github.com/ModelTC/Qwen-Image-Lightning), developed by [Lightx2v](https://github.com/ModelTC/LightX2V), provides [Day 0 acceleration support for Qwen-Image-2512](https://huggingface.co/lightx2v/Qwen-Image-2512-Lightning).
- 2025.12.31:vLLM-Omni supports high performance Qwen-Image-2512 inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check [here](https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/text_to_image) for details.
- 2025.12.23: We released Qwen-Image-Edit-2511 weights! Check at [Huggingface](https://huggingface.co/Qwen/Qwen-Image-Edit-2511) and [ModelScope](https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511)!
- 2025.12.23: We released Qwen-Image-Edit-2511! Check our [Blog](https://qwen.ai/blog?id=qwen-image-edit-2511) for more details!
- 2025.12.23: **[LightX2V](https://github.com/ModelTC/LightX2V/)** delivers Day 0 acceleration for Qwen-Image-Edit-2511, with native support for a wide range of hardware, including **NVIDIA, Hygon, Metax, Ascend, and Cambricon**. By combining **[diffusion distillation](https://github.com/ModelTC/Qwen-Image-Lightning)** with cutting-edge inference optimizations, LightX2V achieves a **25x reduction in DiT NFEs** and **an order-of-magnitude 42.55x overall speedup**, enabling real-time image editing across diverse AI accelerators.
- 2025.12.23: **vLLM-Omni** supports high performance `Qwen-Image-Edit-2511`, `Qwen-Image-Layered` inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check [here](https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/image_to_image) for details.

- 2025.12.23: **SGLang-Diffusion** provides day-0 support for Qwen-Image models. To play with `Qwen-Image-Edit-2511` in SGlang, please check community supports section for details.

- 2025.12.19: We released Qwen-Image-Layered weights! Check at [Huggingface](https://huggingface.co/Qwen/Qwen-Image-Layered) and [ModelScope](https://modelscope.cn/models/Qwen/Qwen-Image-Layered)!
- 2025.12.19: We released Qwen-Image-Layered! Check our [Blog](https://qwenlm.github.io/blog/qwen-image-layered) for more details!
- 2025.12.18: We released our [Research Paper](https://arxiv.org/abs/2512.15603) on Arxiv!
- 2025.11.11: **[T2I-CoreBench](https://t2i-corebench.github.io/)** offers a comprehensive and complex evaluation of T2I models in real-world scenarios. On this benchmark, Qwen-Image achieves state-of-the-art performance under real-world complexities in both composition and reasoning T2I tasks, surpassing other open-source models and showing comparable results to closed-source ones.
- 2025.11.07: LeMiCa is a diffusion model inference acceleration solution developed by China Unicom Data Science and Artificial Intelligence Research Institute. By leveraging cache-based techniques and global denoising path optimization, LeMiCa provides efficient inference support for Qwen-Image, achieving nearly 3x lossless acceleration while maintaining visual consistency and quality. For more details, please visit the homepage: [https://unicomai.github.io/LeMiCa/](https://unicomai.github.io/LeMiCa/)

- 2025.09.22: This September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit [Qwen Chat](https://qwen.ai)  and select the "Image Editing" feature. Compared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:

- 2025.08.19: We have observed performance misalignments of Qwen-Image-Edit. To ensure optimal results, please update to the latest diffusers commit. Improvements are expected, especially in identity preservation and instruction following.
- 2025.08.18: Weâ€™re excited to announce the open-sourcing of Qwen-Image-Edit! ğŸ‰ Try it out in your local environment with the quick start guide below, or head over to [Qwen Chat](https://chat.qwen.ai/) or [Huggingface Demo](https://huggingface.co/spaces/Qwen/Qwen-Image-Edit) to experience the online demo right away! If you enjoy our work, please show your support by giving our repository a star. Your encouragement means a lot to us!
- 2025.08.09: Qwen-Image now supports a variety of LoRA models, such as MajicBeauty LoRA, enabling the generation of highly realistic beauty images. Check out the available weights on [ModelScope](https://modelscope.cn/models/merjic/majicbeauty-qwen1/summary).
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/magicbeauty.png#center)
    
- 2025.08.05: Qwen-Image is now natively supported in ComfyUI, see [Qwen-Image in ComfyUI: New Era of Text Generation in Images!](https://blog.comfy.org/p/qwen-image-in-comfyui-new-era-of)
- 2025.08.05: Qwen-Image is now on Qwen Chat. Click [Qwen Chat](https://chat.qwen.ai/) and choose "Image Generation".
- 2025.08.05: We released our [Technical Report](https://arxiv.org/abs/2508.02324) on Arxiv!
- 2025.08.04: We released Qwen-Image weights! Check at [Huggingface](https://huggingface.co/Qwen/Qwen-Image) and [ModelScope](https://modelscope.cn/models/Qwen/Qwen-Image)!
- 2025.08.04: We released Qwen-Image! Check our [Blog](https://qwenlm.github.io/blog/qwen-image) for more details!

> [!NOTE]
> Due to heavy traffic, if you'd like to experience our demo online, we also recommend visiting DashScope, WaveSpeed, and LibLib. Please find the links below in the community support.

## Quick Start

1. Make sure your transformers>=4.51.3 (Supporting Qwen2.5-VL)

2. Install the latest version of diffusers
```
pip install git+https://github.com/huggingface/diffusers
```

### Qwen-Image-2512 (for Text to Image generation, better character realism/texture quality)

We recommand use the latest prompt enhancing tools for Qwen-Image-2512, please check `src/examples/tools/prompt_utils_2512.py`

```python
from diffusers import QwenImagePipeline
import torch
# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = QwenImagePipeline.from_pretrained("Qwen/Qwen-Image-2512", torch_dtype=torch_dtype).to(device)

# Generate image
prompt = '''A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyesâ€”expressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colorsâ€”lightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illuminationâ€”no staged lightingâ€”and the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.'''

negative_prompt = "ä½åˆ†è¾¨ç‡ï¼Œä½ç”»è´¨ï¼Œè‚¢ä½“ç•¸å½¢ï¼Œæ‰‹æŒ‡ç•¸å½¢ï¼Œç”»é¢è¿‡é¥±å’Œï¼Œèœ¡åƒæ„Ÿï¼Œäººè„¸æ— ç»†èŠ‚ï¼Œè¿‡åº¦å…‰æ»‘ï¼Œç”»é¢å…·æœ‰AIæ„Ÿã€‚æ„å›¾æ··ä¹±ã€‚æ–‡å­—æ¨¡ç³Šï¼Œæ‰­æ›²ã€‚"


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")

```


### Qwen-Image-Edit-2511 (for Image Editing, Multiple Image Support and Improved Consistency)

```python
import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2511", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2511/edit2511input.png").content))
prompt = "è¿™ä¸ªå¥³ç”Ÿçœ‹ç€é¢å‰çš„ç”µè§†å±å¹•ï¼Œå±å¹•ä¸Šé¢å†™ç€â€œé˜¿é‡Œå·´å·´â€"
inputs = {
    "image": [image1],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_2511.png")
    print("image saved at", os.path.abspath("output_image_edit_2511.png"))
```

<details>
<summary> Previous Version </summary>

### Qwen-Image (for Text-to-Image)

The following contains a code snippet illustrating how to use the model to generate images based on text prompts:

```python
from diffusers import DiffusionPipeline
import torch

model_name = "Qwen/Qwen-Image"

# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype).to(device)

positive_magic = {
    "en": ", Ultra HD, 4K, cinematic composition.", # for english prompt
    "zh": ", è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾." # for chinese prompt
}

# Generate image
prompt = '''A coffee shop entrance features a chalkboard sign reading "Qwen Coffee ğŸ˜Š $2 per cup," with a neon light beside it displaying "é€šä¹‰åƒé—®". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written "Ï€â‰ˆ3.1415926-53589793-23846264-33832795-02384197".'''

negative_prompt = " " # Recommended if you don't use a negative prompt.


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt + positive_magic["en"],
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")
```

### Qwen-Image-Edit (for Image Editing, Only Support Single Image Input)
> [!NOTE]
> Qwen-Image-Edit-2509 has better consistency than Qwen-Image-Edit; it is recommended to use Qwen-Image-Edit-2509 directlyï¼Œfor both single image input and multiple image inputs.


```python
import os
from PIL import Image
import torch

from diffusers import QwenImageEditPipeline

pipeline = QwenImageEditPipeline.from_pretrained("Qwen/Qwen-Image-Edit")
print("pipeline loaded")
pipeline.to(torch.bfloat16)
pipeline.to("cuda")
pipeline.set_progress_bar_config(disable=None)

image = Image.open("./input.png").convert("RGB")
prompt = "Change the rabbit's color to purple, with a flash light background."


inputs = {
    "image": image,
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 50,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit.png")
    print("image saved at", os.path.abspath("output_image_edit.png"))
```



> [!NOTE]
> We have observed that editing results may become unstable if prompt rewriting is not used. Therefore, we strongly recommend applying prompt rewriting to improve the stability of editing tasks. For reference, please see our official [demo script](src/examples/tools/prompt_utils.py) or Advanced Usage below, which includes example system prompts. Qwen-Image-Edit is actively evolving with ongoing development. Stay tuned for future enhancements!



### Qwen-Image-Edit-2509 (for Image Editing, Multiple Image Support and Improved Consistency)

```python
import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_1.jpg").content))
image2 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg").content))
prompt = "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."
inputs = {
    "image": [image1, image2],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_plus.png")
    print("image saved at", os.path.abspath("output_image_edit_plus.png"))
```
</details>

### Advanced Usage

#### Prompt Enhance for Text-to-Image
For enhanced prompt optimization and multi-language support, we recommend using our official Prompt Enhancement Tool powered by Qwen-Plus .

You can integrate it directly into your code:
```python
from tools.prompt_utils import rewrite
prompt = rewrite(prompt)
```

Alternatively, run the example script from the command line:

```bash
cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx python examples/generate_w_prompt_enhance.py
```

#### Prompt Enhance for Image Edit
For enhanced stability, we recommend using our official Prompt Enhancement Tool powered by Qwen-VL-Max.

You can integrate it directly into your code:
```python
from tools.prompt_utils import polish_edit_prompt
prompt = polish_edit_prompt(prompt, pil_image)
```


## Deploy Qwen-Image

Qwen-Image supports Multi-GPU API Server for local deployment:

### Multi-GPU API Server Pipeline & Usage

The Multi-GPU API Server will start a Gradio-based web interface with:
- Multi-GPU parallel processing
- Queue management for high concurrency
- Automatic prompt optimization
- Support for multiple aspect ratios

Configuration via environment variables:
```bash
export NUM_GPUS_TO_USE=4          # Number of GPUs to use
export TASK_QUEUE_SIZE=100        # Task queue size
export TASK_TIMEOUT=300           # Task timeout in seconds
```

```bash
# Start the gradio demo server, api key for prompt enhance
cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxx python examples/demo.py 
```


## Showcase
For previous showcases, click the following links:
- [Qwen-Image](./Qwen-Image.md)
- [Qwen-Image-Edit](./Qwen-Image-Edit.md)
- [Qwen-Image-Edit-2509](./Qwen-Image-Edit-2509.md)

### Showcase of Qwen-Image-2512
**Enhanced Huamn Realism**

In Qwen-Image-2512, human depiction has been substantially refined. Compared to the August release, Qwen-Image-2512 adds significantly richer facial details and better environmental context. For example:


> A Chinese female college student, around 20 years old, with a very short haircut that conveys a gentle, artistic vibe. Her hair naturally falls to partially cover her cheeks, projecting a tomboyish yet charming demeanor. She has cool-toned fair skin and delicate features, with a slightly shy yet subtly confident expressionâ€”her mouth crooked in a playful, youthful smirk. She wears an off-shoulder top, revealing one shoulder, with a well-proportioned figure. The image is framed as a close-up selfie: she dominates the foreground, while the background clearly shows her dormitoryâ€”a neatly made bed with white linens on the top bunk, a tidy study desk with organized stationery, and wooden cabinets and drawers. The photo is captured on a smartphone under soft, even ambient lighting, with natural tones, high clarity, and a bright, lively atmosphere full of youthful, everyday energy.

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡1.JPG#center)

For the same prompt, Qwen-Image-2512 yields notably more lifelike facial features, and background objectsâ€”e.g., the desk, stationery, and beddingâ€”are rendered with significantly greater clarity than in Qwen-Image.


> A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyesâ€”expressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colorsâ€”lightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illuminationâ€”no staged lightingâ€”and the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.


![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡2.JPG#center)

Here, hair strands serve as a key differentiator: Qwen-Imageâ€™s August version tends to blur them together, losing fine detail, whereas Qwen-Image-2512 renders individual strands with precision, resulting in a more natural and realistic appearance.

Another case:

> An East Asian teenage boy, aged 15â€“18, with soft, fluffy black short hair and refined facial contours. His large, warm brown eyes sparkle with energy. His fair skin and sunny, open smile convey an approachable, friendly demeanorâ€”no makeup or blemishes. He wears a blue-and-white summer uniform shirt, slightly unbuttoned, made of thin breathable fabric, with black headphones hanging around his neck. His hands are in his pockets, body leaning slightly forward in a relaxed pose, as if engaged in conversation. Behind him lies a summer school playground: lush green grass and a red rubber track in the foreground, blurred school buildings in the distance, a clear blue sky with fluffy white clouds. The bright, airy lighting evokes a joyful, carefree adolescent atmosphere.



![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡3.JPG#center)

In this example, Qwen-Image-2512 better adheres to semantic instructionsâ€”for instance, the prompt specifies â€œbody leaning slightly forward,â€ and Qwen-Image-2512 accurately captures this posture, unlike its predecessor.


> An elderly Chinese couple in their 70s in a clean, organized home kitchen. The woman has a kind face and a warm smile, wearing a patterned apron; the man stands behind her, also smiling, as they both gaze at a steaming pot of buns on the stove. The kitchen is bright and tidy, exuding warmth and harmony. The scene is captured with a wide-angle lens to fully show the subjects and their surroundings.



![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡4.JPG#center)

This comparison starkly highlights the gap between the August and December models. The original Qwen-Image struggles to accurately render aged facial features (e.g., wrinkles), resulting in an artificial â€œAI look.â€ In contrast, Qwen-Image-2512 precisely captures age cues, dramatically boosting realism.



**Finer Natural Detail**

Qwen-Image-2512â€™s enhanced detail rendering extends beyond humansâ€”to landscapes, wildlife, and more. For instance:


> A turquoise river winds through a lush canyon. Thick moss and dense ferns blanket the rocky walls; multiple waterfalls cascade from above, enveloped in mist. At noon, sunlight filters through the dense canopy, dappling the river surface with shimmering light. The atmosphere is humid and fresh, pulsing with primal jungle vitality. No humans, text, or artificial traces present.



![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡5.JPG#center)

Side-by-side, Qwen-Image-2512 exhibits superior fidelity in water flow, foliage, and waterfall mistâ€”and renders richer gradation in greens. Another example (wave rendering):


> At dawn, a thin mist veils the sea. An ancient stone lighthouse stands at the cliffâ€™s edge, its beacon faintly visible through the fog. Black rocks are pounded by waves, sending up bursts of white spray. The sky glows in soft blue-purple hues under cool, hazy lightâ€”evoking solitude and solemn grandeur.



![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡6.JPG#center)

Fur detail is another highlightâ€”here, a golden retriever portrait:


> An ultra-realistic close-up of a golden retriever outdoors under soft daylight. Hair is exquisitely detailed: strands distinct, color transitioning naturally from warm gold to light cream, light glinting delicately at the tips; a gentle breeze adds subtle volume. Undercoat is soft and dense; guard hairs are long and well-defined, with visible layering. Eyes are moist, expressive; nose is slightly damp with fine specular highlights. Background is softly blurred to emphasize the dogâ€™s tangible texture and vivid expression.


![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡7.JPG#center)




Similarly, texture quality improves in depictions of rugged wildlifeâ€”for example, a male argali sheep:


> A male argali stands atop a barren, rocky mountainside. Its coarse, dense grey-brown coat covers a powerful, muscular body. Most striking are its massive, thick, outward-spiraling hornsâ€”a symbol of wild strength. Its gaze is alert and sharp. The background reveals steep alpine terrain: jagged peaks, sparse low vegetation, and abundant sunlightâ€”conveying the harsh yet majestic wilderness and the animalâ€™s resilient vitality.


![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡8.JPG#center)

**Improved Text Rendering**

Qwen-Image-2512 further elevates text renderingâ€”already a strength of the originalâ€”by improving accuracy, layout, and multimodal integration.

For instance, this prompt requests a complete PPT slide illustrating Qwen-Imageâ€™s development roadmap (generation and editing tracks):

> è¿™æ˜¯ä¸€å¼ ç°ä»£é£æ ¼çš„ç§‘æŠ€æ„Ÿå¹»ç¯ç‰‡ï¼Œæ•´ä½“é‡‡ç”¨æ·±è“è‰²æ¸å˜èƒŒæ™¯ã€‚æ ‡é¢˜æ˜¯â€œQwen-Imageå‘å±•å†ç¨‹â€ã€‚ä¸‹æ–¹ä¸€æ¡æ°´å¹³å»¶ä¼¸çš„å‘å…‰æ—¶é—´è½´ï¼Œè½´çº¿ä¸­é—´å†™ç€â€œç”Ÿå›¾è·¯çº¿â€ã€‚ç”±å·¦ä¾§æ·¡è“è‰²æ¸å˜ä¸ºå³ä¾§æ·±ç´«è‰²ï¼Œå¹¶ä»¥ç²¾è‡´çš„ç®­å¤´æ”¶å°¾ã€‚æ—¶é—´è½´ä¸Šæ¯ä¸ªèŠ‚ç‚¹é€šè¿‡è™šçº¿è¿æ¥è‡³ä¸‹æ–¹é†’ç›®çš„è“è‰²åœ†è§’çŸ©å½¢æ—¥æœŸæ ‡ç­¾ï¼Œæ ‡ç­¾å†…ä¸ºæ¸…æ™°ç™½è‰²å­—ä½“ï¼Œä»å·¦å‘å³ä¾æ¬¡å†™ç€ï¼šâ€œ2025å¹´5æœˆ6æ—¥ Qwen-Image é¡¹ç›®å¯åŠ¨â€â€œ2025å¹´8æœˆ4æ—¥  Qwen-Image å¼€æºå‘å¸ƒâ€â€œ2025å¹´12æœˆ31æ—¥ Qwen-Image-2512 å¼€æºå‘å¸ƒâ€ ï¼ˆå‘¨å›´å…‰æ™•æ˜¾è‘—ï¼‰åœ¨ä¸‹æ–¹ä¸€æ¡æ°´å¹³å»¶ä¼¸çš„å‘å…‰æ—¶é—´è½´ï¼Œè½´çº¿ä¸­é—´å†™ç€â€œç¼–è¾‘è·¯çº¿â€ã€‚ç”±å·¦ä¾§æ·¡è“è‰²æ¸å˜ä¸ºå³ä¾§æ·±ç´«è‰²ï¼Œå¹¶ä»¥ç²¾è‡´çš„ç®­å¤´æ”¶å°¾ã€‚æ—¶é—´è½´ä¸Šæ¯ä¸ªèŠ‚ç‚¹é€šè¿‡è™šçº¿è¿æ¥è‡³ä¸‹æ–¹é†’ç›®çš„è“è‰²åœ†è§’çŸ©å½¢æ—¥æœŸæ ‡ç­¾ï¼Œæ ‡ç­¾å†…ä¸ºæ¸…æ™°ç™½è‰²å­—ä½“ï¼Œä»å·¦å‘å³ä¾æ¬¡å†™ç€ï¼šâ€œ2025å¹´8æœˆ18æ—¥ Qwen-Image-Edit å¼€æºå‘å¸ƒâ€â€œ2025å¹´9æœˆ22æ—¥ Qwen-Image-Edit-2509 å¼€æºå‘å¸ƒâ€â€œ2025å¹´12æœˆ19æ—¥ Qwen-Image-Layered å¼€æºå‘å¸ƒâ€â€œ2025å¹´12æœˆ23æ—¥ Qwen-Image-Edit-2511 å¼€æºå‘å¸ƒâ€

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡9.JPG#center)

We can even generate a before-and-after comparison slide to highlight the leap from â€œAI-blurryâ€ to â€œphotorealisticâ€:


> è¿™æ˜¯ä¸€å¼ ç°ä»£é£æ ¼çš„ç§‘æŠ€æ„Ÿå¹»ç¯ç‰‡ï¼Œæ•´ä½“é‡‡ç”¨æ·±è“è‰²æ¸å˜èƒŒæ™¯ã€‚é¡¶éƒ¨ä¸­å¤®ä¸ºç™½è‰²æ— è¡¬çº¿ç²—ä½“å¤§å­—æ ‡é¢˜â€œQwen-Image-2512é‡ç£…å‘å¸ƒâ€ã€‚ç”»é¢ä¸»ä½“ä¸ºæ¨ªå‘å¯¹æ¯”å›¾ï¼Œè§†è§‰ç„¦ç‚¹é›†ä¸­äºä¸­é—´çš„å‡çº§å¯¹æ¯”åŒºåŸŸã€‚å·¦ä¾§ä¸ºé¢éƒ¨å…‰æ»‘æ²¡æœ‰ä»»ä½•ç»†èŠ‚çš„å¥³æ€§äººåƒï¼Œè´¨æ„Ÿå·®ï¼›å³ä¾§ä¸ºé«˜åº¦å†™å®çš„å¹´è½»å¥³æ€§è‚–åƒï¼Œçš®è‚¤å‘ˆç°çœŸå®æ¯›å­”çº¹ç†ä¸ç»†å¾®å…‰å½±å˜åŒ–ï¼Œå‘ä¸æ ¹æ ¹åˆ†æ˜ï¼Œçœ¼çœ¸é€äº®ï¼Œè¡¨æƒ…è‡ªç„¶ï¼Œæ•´ä½“è´¨æ„Ÿæ¥è¿‘å†™å®æ‘„å½±ã€‚ä¸¤å›¾åƒä¹‹é—´ä»¥ä¸€ä¸ªç»¿è‰²æµçº¿å‹ç®­å¤´é“¾æ¥ã€‚é€ å‹ç§‘æŠ€æ„Ÿåè¶³ï¼Œä¸­éƒ¨æ ‡æ³¨â€œ2512è´¨æ„Ÿå‡çº§â€ï¼Œä½¿ç”¨ç™½è‰²åŠ ç²—å­—ä½“ï¼Œå±…ä¸­æ˜¾ç¤ºã€‚ç®­å¤´ä¸¤ä¾§æœ‰å¾®å¼±å…‰æ™•æ•ˆæœï¼Œå¢å¼ºåŠ¨æ€æ„Ÿã€‚åœ¨å›¾åƒä¸‹æ–¹ï¼Œä»¥ç™½è‰²æ–‡å­—å‘ˆç°ä¸‰è¡Œè¯´æ˜ï¼šâ€œâ— æ›´çœŸå®çš„äººç‰©è´¨æ„Ÿã€‚å¤§å¹…åº¦é™ä½äº†ç”Ÿæˆå›¾ç‰‡çš„AIæ„Ÿï¼Œæå‡äº†å›¾åƒçœŸå®æ€§ â— æ›´ç»†è…»çš„è‡ªç„¶çº¹ç†ã€‚å¤§å¹…åº¦æå‡äº†ç”Ÿæˆå›¾ç‰‡çš„çº¹ç†ç»†èŠ‚ã€‚é£æ™¯å›¾ï¼ŒåŠ¨ç‰©æ¯›å‘åˆ»ç”»æ›´ç»†è…»ã€‚â— æ›´å¤æ‚çš„æ–‡å­—æ¸²æŸ“ã€‚å¤§å¹…æå‡äº†æ–‡å­—æ¸²æŸ“çš„è´¨é‡ã€‚å›¾æ–‡æ··åˆæ¸²æŸ“æ›´å‡†ç¡®ï¼Œæ’ç‰ˆæ›´å¥½â€

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡10.JPG#center)

A more complex infographic example:



> è¿™æ˜¯ä¸€å¹…ä¸“ä¸šçº§å·¥ä¸šæŠ€æœ¯ä¿¡æ¯å›¾è¡¨ï¼Œæ•´ä½“é‡‡ç”¨æ·±è“è‰²ç§‘æŠ€æ„ŸèƒŒæ™¯ï¼Œå…‰çº¿å‡åŒ€æŸ”å’Œï¼Œè¥é€ å‡ºå†·é™ã€ç²¾å‡†çš„ç°ä»£å·¥ä¸šæ°›å›´ã€‚ç”»é¢åˆ†ä¸ºå·¦å³ä¸¤å¤§æ¿å—ï¼Œå¸ƒå±€æ¸…æ™°ï¼Œè§†è§‰å±‚æ¬¡åˆ†æ˜ã€‚å·¦ä¾§æ¿å—æ ‡é¢˜ä¸ºâ€œå®é™…å‘ç”Ÿçš„ç°è±¡â€ï¼Œä»¥æµ…è“è‰²åœ†è§’çŸ©å½¢æ¡†çªå‡ºæ˜¾ç¤ºï¼Œå†…éƒ¨æ’åˆ—ä¸‰ä¸ªæ·±è“è‰²æŒ‰é’®å¼æ¡ç›®ï¼Œç¬¬ä¸€ä¸ªæ¡ç›®å±•ç¤ºä¸€å †æ£•è‰²ç²‰æœ«çŠ¶åŸæ–™ä¸Šæ»´è½æ°´æ»´çš„å›¾æ ‡ï¼Œæ–‡å­—ä¸ºâ€œå›¢èš/ç»“å—â€ï¼Œåé¢é…æœ‰ç»¿è‰²å¯¹é’©ï¼›ç¬¬äºŒä¸ªæ¡ç›®ä¸ºä¸€ä¸ªè£…æœ‰è“è‰²æ¶²ä½“å¹¶å†’å‡ºæ°”æ³¡çš„é”¥å½¢ç“¶ï¼Œæ–‡å­—ä¸ºâ€œäº§ç”Ÿæ°”æ³¡/ç¼ºé™·â€ï¼Œåé¢é…æœ‰ç»¿è‰²å¯¹é’©ï¼›ç¬¬ä¸‰ä¸ªæ¡ç›®ä¸ºä¸¤ä¸ªç”Ÿé”ˆçš„é½¿è½®ï¼Œæ–‡å­—ä¸ºâ€œè®¾å¤‡è…èš€/å‚¬åŒ–å‰‚å¤±æ´»â€ï¼Œåé¢é…æœ‰ç»¿è‰²å¯¹é’©ã€‚å³ä¾§æ¿å—æ ‡é¢˜ä¸ºâ€œã€ä¸ä¼šã€‘å‘ç”Ÿçš„ç°è±¡â€ï¼Œä½¿ç”¨ç±³é»„è‰²åœ†è§’çŸ©å½¢æ¡†å‘ˆç°ï¼Œå†…éƒ¨å››ä¸ªæ¡ç›®å‡ç½®äºæ·±ç°è‰²èƒŒæ™¯æ–¹æ¡†ä¸­ã€‚å›¾æ ‡åˆ†åˆ«ä¸ºï¼šä¸€ç»„ç²¾å¯†å•®åˆçš„é‡‘å±é½¿è½®ï¼Œæ–‡å­—ä¸ºâ€œååº”æ•ˆç‡ã€æ˜¾è‘—æé«˜ã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ï¼›ä¸€æ†æ•´é½æ’åˆ—çš„é‡‘å±ç®¡æï¼Œæ–‡å­—ä¸ºâ€œæˆå“å†…éƒ¨ã€ç»å¯¹æ— æ°”æ³¡/å­”éš™ã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ï¼›ä¸€æ¡åšå›ºçš„é‡‘å±é“¾æ¡æ­£åœ¨æ‰¿å—æ‹‰åŠ›ï¼Œæ–‡å­—ä¸ºâ€œææ–™å¼ºåº¦ä¸è€ä¹…æ€§ã€å¾—åˆ°å¢å¼ºã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ï¼›ä¸€å †è…èš€çš„æ‰³æ‰‹ï¼Œæ–‡å­—ä¸ºâ€œåŠ å·¥è¿‡ç¨‹ã€é›¶è…èš€/é›¶å‰¯ååº”é£é™©ã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ã€‚åº•éƒ¨ä¸­å¤®æœ‰ä¸€è¡Œå°å­—æ³¨é‡Šï¼šâ€œæ³¨ï¼šæ°´åˆ†çš„å­˜åœ¨é€šå¸¸ä¼šå¯¼è‡´è´Ÿé¢æˆ–å¹²æ‰°æ€§çš„ç»“æœï¼Œè€Œéç†æƒ³æˆ–å¢å¼ºçš„çŠ¶æ€â€ï¼Œå­—ä½“ä¸ºç™½è‰²ï¼Œæ¸…æ™°å¯è¯»ã€‚æ•´ä½“é£æ ¼ç°ä»£ç®€çº¦ï¼Œé…è‰²å¯¹æ¯”å¼ºçƒˆï¼Œå›¾å½¢ç¬¦å·å‡†ç¡®ä¼ è¾¾æŠ€æœ¯é€»è¾‘ï¼Œé€‚åˆç”¨äºå·¥ä¸šåŸ¹è®­æˆ–ç§‘æ™®æ¼”ç¤ºåœºæ™¯ã€‚

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡11.JPG#center)

Or even a full educational poster:


> è¿™æ˜¯ä¸€å¹…ç”±åäºŒä¸ªåˆ†æ ¼ç»„æˆçš„3Ã—4ç½‘æ ¼å¸ƒå±€çš„å†™å®æ‘„å½±ä½œå“ï¼Œæ•´ä½“å‘ˆç°â€œå¥åº·çš„ä¸€å¤©â€ä¸»é¢˜ï¼Œç”»é¢é£æ ¼ç®€æ´æ¸…æ™°ï¼Œæ¯ä¸€åˆ†æ ¼ç‹¬ç«‹æˆæ™¯åˆç»Ÿä¸€äºç”Ÿæ´»èŠ‚å¥çš„å™äº‹è„‰ç»œã€‚ç¬¬ä¸€è¡Œåˆ†åˆ«æ˜¯â€œ06:00 æ™¨è·‘å”¤é†’èº«ä½“â€ï¼šé¢éƒ¨ç‰¹å†™ï¼Œä¸€ä½å¥³æ€§èº«ç©¿ç°è‰²è¿åŠ¨å¥—è£…ï¼ŒèƒŒæ™¯æ˜¯åˆå‡çš„æœé˜³ä¸è‘±éƒç»¿æ ‘ï¼›â€œ06:30 åŠ¨æ€æ‹‰ä¼¸æ¿€æ´»å…³èŠ‚â€ï¼šå¥³æ€§èº«ç€ç‘œä¼½æœåœ¨é˜³å°åšæ™¨é—´æ‹‰ä¼¸ï¼Œèº«ä½“èˆ’å±•ï¼ŒèƒŒæ™¯ä¸ºæ·¡ç²‰è‰²å¤©ç©ºä¸è¿œå±±è½®å»“ï¼›â€œ07:30 å‡è¡¡è¥å…»æ—©é¤â€ï¼šæ¡Œä¸Šæ‘†æ”¾å…¨éº¦é¢åŒ…ã€ç‰›æ²¹æœå’Œä¸€æ¯æ©™æ±ï¼Œå¥³æ€§å¾®ç¬‘ç€å‡†å¤‡ç”¨é¤ï¼›â€œ08:00 è¡¥æ°´æ¶¦ç‡¥â€ï¼šé€æ˜ç»ç’ƒæ°´æ¯ä¸­æµ®æœ‰æŸ æª¬ç‰‡ï¼Œå¥³æ€§æ‰‹æŒæ°´æ¯è½»å•œï¼Œé˜³å…‰ä»å·¦ä¾§æ–œç…§å…¥å®¤ï¼Œæ¯å£æ°´ç æ»‘è½ï¼›ç¬¬äºŒè¡Œåˆ†åˆ«æ˜¯ï¼šâ€œ09:00 ä¸“æ³¨é«˜æ•ˆå·¥ä½œâ€ï¼šå¥³æ€§ä¸“æ³¨æ•²å‡»é”®ç›˜ï¼Œå±å¹•æ˜¾ç¤ºç®€æ´ç•Œé¢ï¼Œèº«æ—æ”¾æœ‰ä¸€æ¯å’–å•¡ä¸ä¸€ç›†ç»¿æ¤ï¼›â€œ12:00 é™å¿ƒé˜…è¯»æ—¶å…‰â€ï¼šå¥³æ€§ååœ¨ä¹¦æ¡Œå‰ç¿»é˜…çº¸è´¨ä¹¦ç±ï¼Œå°ç¯æ•£å‘æš–å…‰ï¼Œä¹¦é¡µæ³›é»„ï¼Œæ—æ”¾åŠæ¯çº¢èŒ¶ï¼›â€œ12:30 åˆåè½»æ¾æ¼«æ­¥â€ï¼šå¥³æ€§åœ¨æ—è«é“ä¸Šæ¼«æ­¥ï¼Œè„¸éƒ¨ç‰¹å†™ï¼›â€œ15:00 èŒ¶é¦™ä¼´åˆåâ€ï¼šå¥³æ€§ç«¯ç€éª¨ç“·èŒ¶æ¯ç«™åœ¨çª—è¾¹ï¼Œçª—å¤–æ˜¯åŸå¸‚è¡—æ™¯ä¸é£˜åŠ¨äº‘æœµï¼ŒèŒ¶é¦™è¢…è¢…ï¼›ç¬¬ä¸‰è¡Œåˆ†åˆ«æ˜¯ï¼šâ€œ18:00 è¿åŠ¨é‡Šæ”¾å‹åŠ›â€ï¼šå¥èº«æˆ¿å†…ï¼Œå¥³æ€§æ­£åœ¨ç»ƒä¹ ç‘œä¼½ï¼›â€œ19:00 ç¾å‘³æ™šé¤â€ï¼šå¥³æ€§åœ¨å¼€æ”¾å¼å¨æˆ¿ä¸­åˆ‡èœï¼Œç §æ¿ä¸Šæœ‰ç•ªèŒ„ä¸é’æ¤’ï¼Œé”…ä¸­çƒ­æ°”å‡è…¾ï¼Œç¯å…‰æ¸©æš–ï¼›â€œ21:00 å†¥æƒ³åŠ©çœ â€ï¼šå¥³æ€§ç›˜è…¿ååœ¨æŸ”è½¯åœ°æ¯¯ä¸Šå†¥æƒ³ï¼ŒåŒæ‰‹è½»æ”¾è†ä¸Šï¼Œé—­ç›®å®é™ï¼›â€œ21:30 è¿›å…¥ç¡çœ â€ï¼šå¥³æ€§èººåœ¨åºŠä¸Šä¼‘æ¯ã€‚æ•´ä½“é‡‡ç”¨è‡ªç„¶å…‰çº¿ä¸ºä¸»ï¼Œè‰²è°ƒä»¥æš–ç™½ä¸ç±³ç°ä¸ºåŸºè°ƒï¼Œå…‰å½±å±‚æ¬¡åˆ†æ˜ï¼Œç”»é¢å……æ»¡æ¸©é¦¨çš„ç”Ÿæ´»æ°”æ¯ä¸è§„å¾‹çš„èŠ‚å¥æ„Ÿã€‚

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/å¹»ç¯ç‰‡12.JPG#center)


These are the core enhancements in this update. We hope you enjoy using Qwen-Image-2512!

### Showcase of Qwen-Image-Edit-2511
**Qwen-Image-Edit-2511 Enhances Character Consistency**
In Qwen-Image-Edit-2511, character consistency has been significantly improved. The model can perform imaginative edits based on an input portrait while preserving the identity and visual characteristics of the subject.

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡1.JPG#center)
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡2.JPG#center)
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡3.JPG#center)
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡4.JPG#center)

**Improved Multi-Person Consistency**
While Qwen-Image-Edit-2509 already improved consistency for single-subject editing, Qwen-Image-Edit-2511 further enhances consistency in multi-person group photosâ€”enabling high-fidelity fusion of two separate person images into a coherent group shot:
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡5.JPG#center)
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡6.JPG#center)

**Built-in Support for Community-Created LoRAs**
Since Qwen-Image-Editâ€™s release, the community has developed many creative and high-quality LoRAsâ€”greatly expanding its expressive potential. Qwen-Image-Edit-2511 integrates selected popular LoRAs directly into the base model, unlocking their effects without extra tuning.

For example, Lighting Enhancement LoRA
Realistic lighting control is now achievable out-of-the-box:
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡7.JPG#center)

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡8.JPG#center)

Another example, generating new viewpoints can now be done directly with the base model:

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡9.JPG#center)

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡10.JPG#center)

**Industrial Design Applications**

Weâ€™ve paid special attention to practical engineering scenariosâ€”for instance, batch industrial product design:


![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡11.JPG#center)

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡12.JPG#center)

â€¦and material replacement for industrial components:
![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡13.JPG#center)

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡14.JPG#center)

**Enhanced Geometric Reasoning**
Qwen-Image-Edit-2511 introduces stronger geometric reasoning capabilityâ€”e.g., directly generating auxiliary construction lines for design or annotation purposes:


![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡15.JPG#center)

![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/å¹»ç¯ç‰‡16.JPG#center)



## AI Arena

To comprehensively evaluate the general image generation capabilities of Qwen-Image and objectively compare it with state-of-the-art closed-source APIs, we introduce [AI Arena](https://aiarena.alibaba-inc.com), an open benchmarking platform built on the Elo rating system. AI Arena provides a fair, transparent, and dynamic environment for model evaluation.

In each round, two imagesâ€”generated by randomly selected models from the same promptâ€”are anonymously presented to users for pairwise comparison. Users vote for the better image, and the results are used to update both personal and global leaderboards via the Elo algorithm, enabling developers, researchers, and the public to assess model performance in a robust and data-driven way. AI Arena is now publicly available, welcoming everyone to participate in model evaluations. 

![AI Arena](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/figure_aiarena_website.png)

The latest leaderboard rankings can be viewed at [AI Arena Learboard](https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=text2image).

If you wish to deploy your model on AI Arena and participate in the evaluation, please contact weiyue.wy@alibaba-inc.com.

## Community Support

### Huggingface

Diffusers has supported Qwen-Image since day 0. Support for LoRA and finetuning workflows is currently in development and will be available soon.

### ModelScope
* **[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)** provides comprehensive support for Qwen-Image, including low-GPU-memory layer-by-layer offload (inference within 4GB VRAM), FP8 quantization, LoRA / full training.
* **[DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine)** delivers advanced optimizations for Qwen-Image inference and deployment, including FBCache-based acceleration, classifier-free guidance (CFG) parallel, and more.
* **[ModelScope AIGC Central](https://www.modelscope.cn/aigc)** provides hands-on experiences on Qwen Image, including: 
    - [Image Generation](https://www.modelscope.cn/aigc/imageGeneration): Generate high fidelity images using the Qwen Image model.
    - [LoRA Training](https://www.modelscope.cn/aigc/modelTraining): Easily train Qwen Image LoRAs for personalized concepts.

### SGLang

**SGLang-Diffusion** provides day-0 support for Qwen-Image models. To play with `Qwen-Image-Edit-2511`, use the following command:

```
sglang generate --model-path Qwen/Qwen-Image-Edit-2511 --prompt "make the girl in Figure 1 dance with the capybara in Figure 2."  --image-path "https://github.com/lm-sys/lm-sys.github.io/releases/download/test/TI2I_Qwen_Image_Edit_Input.jpg" "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg"
```

The output should be like
![](https://github.com/lm-sys/lm-sys.github.io/releases/download/test/SGLang_Diffusion_Qwen_Image_Edit_2511_example_output.jpg )

### WaveSpeedAI

WaveSpeed has deployed Qwen-Image on their platform from day 0, visit their [model page](https://wavespeed.ai/models/wavespeed-ai/qwen-image/text-to-image) for more details.

### LiblibAI

LiblibAI offers native support for Qwen-Image from day 0. Visit their [community](https://www.liblib.art/modelinfo/c62a103bd98a4246a2334e2d952f7b21?from=sd&versionUuid=75e0be0c93b34dd8baeec9c968013e0c) page for more details and discussions.

### Inference Acceleration Method: cache-dit

cache-dit offers cache acceleration support for Qwen-Image with DBCache, TaylorSeer and Cache CFG. Visit their [example](https://github.com/vipshop/cache-dit/blob/main/examples/pipeline/run_qwen_image.py) for more details.

## License Agreement

Qwen-Image is licensed under Apache 2.0. 

## Citation

We kindly encourage citation of our work if you find it useful.

```bibtex
@misc{wu2025qwenimagetechnicalreport,
      title={Qwen-Image Technical Report}, 
      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},
      year={2025},
      eprint={2508.02324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.02324}, 
}
```


## Contact and Join Us


If you'd like to get in touch with our research team, we'd love to hear from you! Join our [Discord](https://discord.gg/z3GAxXZ9Ce) or scan the QR code to connect via our [WeChat groups](assets/wechat.png) â€” we're always open to discussion and collaboration.

If you have questions about this repository, feedback to share, or want to contribute directly, we welcome your issues and pull requests on GitHub. Your contributions help make Qwen-Image better for everyone. 

If you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns. Don't wait â€” reach out to us at fulai.hr@alibaba-inc.com

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=QwenLM/Qwen-Image&type=Date)](https://www.star-history.com/#QwenLM/Qwen-Image&Date)












